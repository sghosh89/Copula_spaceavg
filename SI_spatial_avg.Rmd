---
title: "Supporting Information : Estimating synchronous and compensatory dynamics in grassland community using a novel “copula” approach"
author: "Shyamolina Ghosh, other authors, Daniel Reuman"
date: "`r format(Sys.time(), '%d %B, %Y')`"
geometry: "left=1.5cm,right=1.5cm,top=1cm,bottom=1.8cm"

output: 
  pdf_document:
    number_sections: yes
    keep_tex: yes
    fig_caption: yes

header-includes:
      - \usepackage{xr} \externaldocument[MT-]{MT_spatial_avg}
      - \input{head_supp.sty}

tables: True
link-citations: True
urlcolor : blue
indent : True
csl: pnas.csl
bibliography: REF_CSS.bib
---

\pagenumbering{roman}
\tableofcontents
\listoftables
\listoffigures

\pagebreak
\pagenumbering{arabic}

<!--Basic setup-->
```{r setup, echo=F, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "H")
seed<-101 # common seed 
surrog_seed1<-103  # seed used to get pearson preserving surrogs
surrog_seed2<-1001 # seed used to get pearson preserving surrogs
library(dplyr)
library(kableExtra)
library(stringr)
library(rmarkdown)
# families<-c(1,3:10,13,14,16:20)
source("mtime.R") #A function needed for caching
```

<!-- reading raw data for hays and creates results folder for hays-->
```{r read hays_rawdata,echo=F,results="hide"}
# later add results="hide" , echo=F in chunk header
set.seed(seed)
# basal cover data
cover<-read.csv("./Data/HaysData/allrecords.csv") # this is basal cover

# quadrat sampling ? Yes or not
quadsamp<-read.csv("./Data/HaysData/quadrat_inventory.csv")

# Information on species list
spinfo<-read.csv("./Data/HaysData/species_list.csv")
nonplant<-as.character(spinfo$species[which(spinfo$type=="remove")]) 
#These are not plant species as per metadata file (page 8 in pdf) for more info: though I doubt on mixed grass and polygonum spp.]
#"Bare ground"    "Fragment"       "Mixed grass"    "Polygonum spp." "Unknown"
spinfo[which(spinfo$type=="remove"),] # for details

#-------creating results folder for hays data------------
if(!dir.exists("./Results/hays_results")){
  dir.create("./Results/hays_results/")
}

if(!dir.exists("./Results/hays_results/skewness_results")){
  dir.create("./Results/hays_results/skewness_results/")
}
```

<!-- preparing hays spatial average data in usual format for common+pseudo sp.-->
```{r prepare_hays_spaceavg,results="hide",echo=F,cache=T, cache.extra=list(seed,cover,spinfo,nonplant)}
# later add results="hide" , echo=F in chunk header
set.seed(seed)
library(stringr)
py<-(cover$plotyear)
plots<-str_sub(string = py,start=1,end=str_length(py)-2) #extracting only plot no.
yrs<-str_sub(string = py,start=-2)        #extracting last 2digit of an year
cover<-cbind(plot=plots,yr=as.numeric(yrs),cover)

uyr<-sort(unique(cover$yr)) # 41 unique years
uplot<-sort(unique(cover$plot)) # 51 unique quadrats
uplot<-as.character(uplot)

splist<-sort(unique(cover$species))
splist<-as.character(splist)  

# check with raw data:
spcount<-as.data.frame(table(cover$species))
checkcount<-(spinfo$count==spcount$Freq)
all(checkcount=T) #These should be true

#sink("./Results/hays_results/hays_myquadsamplinginfo.txt", append=TRUE, split=TRUE)

hays_array<-array(Inf,dim=c(length(uyr),length(uplot),length(splist)),dimnames = list(uyr,uplot,splist))

for(iyr in 1:length(uyr)){
  tempo<-subset(cover,cover$yr==uyr[iyr])
  for(iplot in 1:length(uplot)){
    tempo2<-subset(tempo,tempo$plot==uplot[iplot])
    if(nrow(tempo2)==0){
      cat("iyr = ",iyr, " Year : ",uyr[iyr], "iplot = ",iplot," plot : ",uplot[iplot],"Not surveyed : --NA--","\n")
      hays_array[iyr,iplot,]<-NA # NA means this plot is not surveyed in that given year : so all species should get NA as basal cover
    }else{
      z<-split( tempo2 , f = tempo2$species ) # This will split the data tempo2 by its species levels (which is a factor)
      for(isp in 1:length(z)){
        cs<-sum(z[[isp]]$area)  # if cs=0 that means that sp. is absent for that plot and for that given year 
                                # it will sum over the area of a particular sp. taken from all ID in a quadrat
        hays_array[iyr,iplot,isp]<-cs
      }
    }
  }
}

#check with prepared data
# check1 : Total basal cover reported for any specific year from 1 sq. meter quadrat should be ~10000 cm^2
check_matcover<-apply(hays_array,FUN=sum,MARGIN=c(1,2),na.rm=T)
check_matcover[check_matcover==0]<-NA
# This check_matcover matrix contains entries either NA for plots not surveyed or 9999...value ~10000 cm^2
range(check_matcover,na.rm=T)
#hist(check_matcover,ylim=c(0,5),col="grey",breaks=100) # check with plot

#sink()

#It's a check : There should not be all zeros for all sp on any plot-year combination
s<-apply(hays_array,FUN = sum,MARGIN = c(1,2)) #This matrix should contain either NA or some +ve number
any(s==0,na.rm = T)  #There should not be any zeros

saveRDS(hays_array,"./Results/hays_results/hays_array_41yr_51plot_151sp.RDS")

# Now calculate average basal cover over all plots for any given year for all species
avg_cover_over_plots_hays<-matrix(Inf,nrow=length(uyr),ncol=length(splist))
rownames(avg_cover_over_plots_hays)<-c(1932:1972)
colnames(avg_cover_over_plots_hays)<-splist
avg_cover_over_plots_hays<-as.data.frame(avg_cover_over_plots_hays)

#check2:

#from raw data
plotsurveyed<-rowSums(!is.na(quadsamp))-1 

#from prepared data
isp<-1 # This should be same for any species
numplot_surveyed_by_year<-apply(hays_array[,,isp], MARGIN = 1, function(x){sum(is.finite(x))}) 

all((plotsurveyed==numplot_surveyed_by_year)==T) #They are same.

for(isp in c(1:length(splist))){
  m<-hays_array[,,isp]
  idNA<-which(apply(m, 1, function(x) all(is.na(x)))) #This should be empty : means should not be all NA's along any row
  if(length(idNA)==0){
    cs<-apply(m,FUN = mean,MARGIN = 1,na.rm=T)
    avg_cover_over_plots_hays[,isp]<-cs
  }else{
    cat("Caution : This means for a given year not any plot was surveyed!!!")
  }
}
anyNA(avg_cover_over_plots_hays) #This should be FALSE

#---------saving a big matrix 41yrs by 151 sp. which has each sp (including non-plant) timeseries along each column : hays data------
saveRDS(avg_cover_over_plots_hays,"./Results/hays_results/timeseries_matrix_spatialavg_allsp_hays.RDS")

# Now screen the species for hays
count0<-apply(avg_cover_over_plots_hays,2,function(x){sum(x==0)}) # This is the count of zero 
count0_allsp<-as.data.frame(count0)

id_nonplant<-which(colnames(avg_cover_over_plots_hays)%in%nonplant)

id_common_sp_hays<-which(count0_allsp$count0<=6)
id_common_sp_hays<-setdiff(id_common_sp_hays,id_nonplant)

id_rare_sp_hays<-which(count0_allsp$count0>=39)
id_rare_sp_hays<-setdiff(id_rare_sp_hays,id_nonplant)

id_interm_sp_hays<-which(count0_allsp$count0<39 & count0_allsp>6)
id_interm_sp_hays<-setdiff(id_interm_sp_hays,id_nonplant)

# check
tot_sp_hays<-length(id_common_sp_hays)+length(id_interm_sp_hays)+length(id_rare_sp_hays)+length(id_nonplant)
tot_sp_hays==151

sp_category_hays<-as.data.frame(matrix(NA,nrow=length(splist),ncol=3))
colnames(sp_category_hays)<-c("id","sp","category")
sp_category_hays$id<-c(1:length(splist))
sp_category_hays$sp<-splist
sp_category_hays$category[id_common_sp_hays]<-"C" #common sp. for Hays data
sp_category_hays$category[id_interm_sp_hays]<-"I" # Intermediate sp. for Hays data
sp_category_hays$category[id_rare_sp_hays]<-"R" # rare sp. for Hays data

#---------saving a 151sp by 2 matrix indicating C/I/R category for each hays-sp (for non-plant = NA)---------
saveRDS(sp_category_hays,"./Results/hays_results/all_sp_category_spatialavg_hays.RDS")

# Now make hays_spaceavg data in your format
hays_spaceavg<-vector("list",1)
names(hays_spaceavg)<-"avg.basal.cover"

sp.screened.data<-vector("list",length(id_common_sp_hays))
names(sp.screened.data)<-splist[id_common_sp_hays]

for(isp in 1:length(id_common_sp_hays)){
  sp.screened.data[[isp]]<-data.frame(Year=c(1932:1972),Dat=avg_cover_over_plots_hays[,id_common_sp_hays[isp]])
}

hays_spaceavg$avg.basal.cover<-sp.screened.data

# Append the pseudo species = merged sp. of I & R category
pseudo_hays_IR<-apply(X=avg_cover_over_plots_hays[,which(sp_category_hays$category%in%c("I","R"))],MARGIN = 1,FUN = sum)
pseudo_hays<-data.frame(Year=c(1932:1972),Dat=pseudo_hays_IR)
pseudo_hays<-list(pseudo_hays)

hays_spaceavg$avg.basal.cover<-append(hays_spaceavg$avg.basal.cover,pseudo_hays)
names(hays_spaceavg$avg.basal.cover)[[length(id_common_sp_hays)+1]]<-"pseudo_hays"

#---------saving the spatial avg. data for hays with whigh we will do taildep. analysis later----------------
saveRDS(hays_spaceavg,"./Results/hays_results/hays_spaceavg_data_CP.RDS")

#-----------saving a matrix with timeseries of common + 1 pseudo (all other merged into 1) sp. for hays------
ts_mat_CP_hays<-cbind(avg_cover_over_plots_hays[,id_common_sp_hays],pseudo_hays_IR)
saveRDS(ts_mat_CP_hays,"./Results/hays_results/skewness_results/ts_mat_CP_hays.RDS")

#------plot hays_spaceavg data for all screened sp for all yearspan-------------------------------
pdf("./Results/hays_results/hays_spaceavg_screenedsp_avgcover.pdf",height = 21,width=21)
op<-par(mfrow=c(6,5),mar=c(5,3,3,3))
for (i in c(1:length(hays_spaceavg$avg.basal.cover))){
  plot(hays_spaceavg$avg.basal.cover[[i]]$Year,hays_spaceavg$avg.basal.cover[[i]]$Dat,col=rgb(0,0,1,0.3),pch=19,type="b",
       ylim=c(0,max(hays_spaceavg$avg.basal.cover[[i]]$Dat,na.rm=T)),xlab="Year (1932-1972)")
  abline(h=0)
  n0<-sum(hays_spaceavg$avg.basal.cover[[i]]$Dat==0)
  mtext(paste0("sp = ",i," : ",names(hays_spaceavg$avg.basal.cover)[i],", n0=",n0,sep=""))
}
par(op)
dev.off()

# ---------------------generate copula plots for all selected splist_hays_spaceavg--------------------
source("./vivj_matrix.R")
good_sp<-c(1:length(hays_spaceavg[[1]]))
lensp<-length(good_sp)
pdf("./Results/hays_results/copula_hays_spaceavg.pdf",height=2*lensp,width = 2*lensp)
op<-par(mfrow=c(lensp,lensp),mar=c(3,3,3,3), mgp=c(1.5,0.5,0))
for(i in c(1:lensp)){
  for(j in c(1:lensp)){
    vivj_matrix(d_allsp=hays_spaceavg,loc=1,
                i=good_sp[i],j=good_sp[j],level=0.05,
                ploton=T,onbounds=T,lb=0,ub=0.5)
  }
}
par(op)
dev.off()
```

<!-- non-parametric analysis for cor stat with common + pseudo sp. in hays-->
```{r npa_hays_spaceavg, echo=F, cache=T, cache.extra=list(seed,hays_spaceavg,mtime("NonParamStat.R"),mtime("vivj_matrix.R"),mtime("CopulaFunctions.R"), mtime("CopulaFunctions_flexible.R"))}

set.seed(seed) 
source("./NonParamStat.R")

if(!dir.exists("./Results/hays_results/corstat_hays_spaceavg_results")){
  dir.create("./Results/hays_results/corstat_hays_spaceavg_results")
} 

resloc<-"./Results/hays_results/corstat_hays_spaceavg_results/"

corstat_hays_spaceavg<-multcall(d_allsp=hays_spaceavg,
                           loc=1,
                           resloc=resloc,
                           good_sp=c(1:length(hays_spaceavg[[1]])),
                           nbin=2)

saveRDS(corstat_hays_spaceavg,paste(resloc,file="corstat_hays_spaceavg.RDS",sep=''))
```

<!-- genarating Corl - Coru plots from non-parametric analysis with common + pseudo sp. in hays-->
```{r plot_res_hays_npa_spaceavg, echo=F, results="hide",cache=T,warning=F, cache.extra=list(seed,corstat_hays_spaceavg,mtime("NonParamStat_matrixplot.R"),mtime("mycorrplot.R"))}
set.seed(seed)
source("./NonParamStat_matrixplot.R")

resloc<-"./Results/hays_results/corstat_hays_spaceavg_results/"

CorlmCoru_hays_spaceavg<-NonParamStat_matrixplot(data=corstat_hays_spaceavg,resloc=resloc,tagon=T,type="lower",wd=13,ht=13)
saveRDS(CorlmCoru_hays_spaceavg,paste(resloc,file="CorlmCoru_hays_spaceavg.RDS",sep='')) 
```

<!--get appropriate surrogates for hays with common+pseudo sp.-->
```{r make_surrogs_CP_hays, echo=F, results="hide", warning=F, cache=T, cache.extra=list(surrog_seed1,surrog_seed2,ts_mat_CP_hays,mtime("copmap.R"),mtime("getinv.R"),mtime("getinvrg.R"),mtime("alignranks.R"))}

#***setup for the chunk

library(matrixcalc)
library(mvtnorm)

# functions needed
source("copmap.R")
source("getinv.R")
source("getinvrg.R")
source("alignranks.R")

# result folder to save surrogates 
resloc_surrog_hays<-"./Results/hays_results/skewness_results/pp_surrogs_hays_CP/"
if (!dir.exists(resloc_surrog_hays)){
  dir.create(resloc_surrog_hays)
}

#***For each pair of species, find the preimage of the correlation 
#under the map defined by copmap.R - this is to determine search bounds
#for the subsequent optimization

set.seed(surrog_seed1)

d<-ts_mat_CP_hays
numsp<-dim(d)[2]
p<-seq(from=-1,to=1,by=0.1)
numreps<-250

#receptacle for results, the preimage in allparms[,,2], the range
#specified by the other values
allparms<-array(NA,c(numsp,numsp,3))

#loop through each pair of species
for (c1 in 1:(numsp-1))
{
  print(paste0("c1: ",c1))
  for (c2 in (c1+1):numsp)
  {
    #get the time series for these two species
    x<-d[,c1]
    y<-d[,c2]
    
    #get the (stochastic) map
    thisres<-copmap(x,y,p,numreps,"cor")
    
    #find the iverse range
    allparms[c1,c2,c(1,3)]<-getinvrg(p,thisres,cor(x,y))
    
    #find the actual preimage
    allparms[c1,c2,2]<-getinv(p,thisres,cor(x,y),"median")
    
    #make and save a plot
    mn<-apply(FUN=mean,MARGIN=2,X=thisres)
    quants<-apply(FUN=quantile,MARGIN=2,X=thisres,probs=c(.25,.5,.75))
    pdf(paste0(resloc_surrog_hays,"mapplot_",c1,"_",c2,".pdf"))
    plot(p,mn,type='l',ylim=range(mn,quants))
    lines(p,quants[2,],type='l',lty="dashed")
    lines(p,quants[1,],type='l',lty="dotted")
    lines(p,quants[3,],type='l',lty="dotted")
    lines(range(p),rep(cor(x,y),2))
    lines(rep(allparms[c1,c2,1],2),c(-1,1))
    lines(rep(allparms[c1,c2,2],2),c(-1,1))
    lines(rep(allparms[c1,c2,3],2),c(-1,1))
    dev.off()
  }
}

#make sure the preimage is within the preimage range in all cases,
#and you got a sensible range and a single-point preimage in all cases
if (sum(allparms[,,1]<allparms[,,2],na.rm=TRUE)!=(numsp^2-numsp)/2 ||
    sum(allparms[,,2]<allparms[,,3],na.rm=TRUE)!=(numsp^2-numsp)/2)
{
  stop("Error in make_surrogs_CP_hays: error screening location 1")
}

#***Now look for positive definite in a couple simple ways

#First check to see if we got lucky and the preimage itself is pos def
m1<-allparms[,,2]
m1[is.na(m1)]<-0
m1<-m1+t(m1)
diag(m1)<-1
if (is.positive.semi.definite(m1))
{
  stop("We got lucky in make_surrogs_CP_hays, revisit code: lucky screening location 1") 
} #If m1 is pos semi def, we can skip much of the rest of the chunk and
#do things in a simpler way, so screen for that

#now use nearPD and see if you can get a positive definite matrix between the bounds
hlo<-P2p(t(allparms[,,1]))
hpre<-P2p(t(allparms[,,2]))
hhi<-P2p(t(allparms[,,3]))
hmid<-(hlo+hhi)/2
bestmatNPD<-matrix(as.numeric(Matrix::nearPD(m1,corr=TRUE)$mat),numsp,numsp)
bestmatNPDvect<-P2p(bestmatNPD)
if (sum(bestmatNPDvect>hhi | bestmatNPDvect<hlo)==0)
{
  stop("We got lucky in make_surrogs_CP_hays, revisit code: lucky screening location 2") 
} #if we got a positive definite matrix here, then we may not have to search
#for one below, so screen for that

#***Now search for a positive definite matrix in the space of parameters 
#desribed by allparms

#prepare
set.seed(surrog_seed2)
numpd<-50000
allposdefmats<-matrix(NA,2*numpd,length(hlo)+1)
allposdefmats_rowcount<-1

#the function to optimize which will lead us toward pos def mats, and will also
#record any that are found
fn<-function(h)
{
  res<-min(eigen(p2P(h))$values)
  if (res>0)
  {
    allposdefmats[allposdefmats_rowcount,]<<-c(h,res)
    allposdefmats_rowcount<<-allposdefmats_rowcount+1 #note the use of global variables, watch out!
    print("Found one!")
  }
  return(res)
}

#do a search starting with the preimage itself
optim(hpre,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
      control=list(fnscale=-1)) 

#do a search starting from the midpoint of the preimage range
optim(hmid,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
      control=list(fnscale=-1)) 

#now do a bunch of searches from random start points
while (allposdefmats_rowcount<=numpd)
{
  startvec<-(hhi-hlo)*runif(length(hlo))+hlo
  optim(startvec,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
        control=list(fnscale=-1)) 
}

allposdefmats<-allposdefmats[1:(allposdefmats_rowcount-1),]
saveRDS(allposdefmats,file=paste0(resloc_surrog_hays,"PosDefMats.RDS"))

#***Now find the result that is closest to hpre

allposdefmats<-allposdefmats[,1:(dim(allposdefmats)[2]-1)]
hpremat<-matrix(hpre,dim(allposdefmats)[1],length(hpre),byrow = TRUE)
sizemat<-matrix((hhi-hlo)/2,dim(allposdefmats)[1],length(hlo),byrow = TRUE)
dist<-apply(X=((allposdefmats-hpremat)/sizemat)^2,FUN=sum,MARGIN=1)
besth<-allposdefmats[which(dist==min(dist)),]
if (any(besth<hlo | besth>hhi))
{
  stop("Error in make_surrogs_CP_hays: error screening location 2")
}#the result should be within the bounds - screen for it
bestmat<-p2P(besth) #this is supposed to be my parameter matrix for a normal copula!
if (!is.positive.semi.definite(bestmat))
{
  stop("Error in make_surrogs_CP_hays: error screening location 3")
}#the result should be positive semi-definite - screen for it

#***Now make a plot showing where in the ranges given by hlo to hhi 
#the besth values sit

inds<-order(besth)
pdf(file=paste0(resloc_surrog_hays,"PDResultComparedToBounds.pdf"))
plot(1:length(besth),besth[inds],type='l',ylim=range(besth,hlo,hhi))
lines(1:length(besth),hlo[inds],type='l',lty="dashed")
lines(1:length(besth),hhi[inds],type='l',lty="dashed")
dev.off()

#***Now assess whether you get reasonably pearson-preserving surrogates
#using bestmat

numsimsforcheck<-1000

sims<-rmvnorm((dim(d)[1])*numsimsforcheck,sigma=bestmat)
sims<-aperm(array(sims,c(dim(d)[1],numsimsforcheck,dim(d)[2])),c(1,3,2))
dim(sims)
sd<-d
for (counter in 1:numsp)
{
  sd[,counter]<-sort(d[,counter])
}#need to sort the columns of d to use alignranks
remapd<-alignranks(sd,sims)
holder<-apply(FUN=cor,MARGIN=3,X=remapd)
allcors<-array(holder,c(numsp,numsp,numsimsforcheck))
gtres<-apply(FUN=sum,MARGIN=c(1,2),X=(allcors>array(cor(d),c(numsp,numsp,numsimsforcheck))))
diag(gtres)<-NA
pdf(file=paste0(resloc_surrog_hays,"AssessIfSurrogsPreservePearson.pdf"))
hist(gtres)#very few of these should be outside the range 250-750, 
#and if they are not outside that range you have pretty 
#good surrogates
dev.off()

saveRDS(bestmat,file=paste0(resloc_surrog_hays,"FinalPDParameterMatrix.RDS"))
#This is the final parameter result. If you generate data from a normal copula
#with this parameter matrix and then use alignranks, that should give ok 
#Pearson-preserving surrogates (though I still need to check if CV_com^2
#is preserved.

#***Now produce 100000 surrogates and save for later use

sims<-rmvnorm((dim(d)[1])*100000,sigma=bestmat)
sims<-aperm(array(sims,c(dim(d)[1],100000,dim(d)[2])),c(1,3,2))
surrogs<-alignranks(sd,sims)
saveRDS(surrogs,file=paste0(resloc_surrog_hays,"Surrogates.RDS"))

#***compute CV_com^2 for surrogs and compare to data value
totpop<-apply(FUN=sum,X=surrogs,MARGIN=c(1,3))
allvars<-apply(FUN=var,X=totpop,MARGIN=2)
allmnsq<-apply(FUN=function(x){(mean(x))^2},X=totpop,MARGIN=2)
allCVcomsq<-allvars/allmnsq
totpop<-apply(FUN=sum,X=d,MARGIN=1)
CVd<-var(totpop)/((mean(totpop))^2)
pdf(paste0(resloc_surrog_hays,"HistogramPlotForCVcomsq.pdf"))
hist(allCVcomsq,50)
lines(rep(CVd,2),c(1,1000),col="red")
dev.off()
saveRDS(sum(allCVcomsq<CVd)/length(allCVcomsq),
        paste0(resloc_surrog_hays,"FracSurrogCVcomsqLTactual.RDS"))

surrogs_CP_hays<-surrogs  # this is the pearson preserving surrogs for 
#use in subsequent chunks
```

<!-- genarating stability based results and plots for hays-->
```{r skewness_hays_spaceavg_PP, echo=F, results="hide", warning=F, cache=T, cache.extra=list(seed,ts_mat_CP_hays,surrogs_CP_hays,mtime("make_tab_stability_assessment.R"),mtime("mycvsq.R"),mtime("SkewnessAnd3CentMom.R"))}

set.seed(seed)
source("make_tab_stability_assessment.R")

# randomly sample numsurrog surrogate matrices from Pearson preserving array
numsurrog<-10000
id_surrogs<-sample(c(1:dim(surrogs_CP_hays)[3]),numsurrog,replace=F)
surrogs_CP_hays<-surrogs_CP_hays[,,id_surrogs]

stability_CP_hays<-make_tab_stability(m=ts_mat_CP_hays,surrogs = surrogs_CP_hays, surrogs_given = T)
ans<-(stability_CP_hays$df_stability)
rownames(ans)<-"C+P"
class(ans)

write.csv(ans,"./Results/hays_results/skewness_results/hays_stability_CP.csv")

#--------------generate plots with hays stability results : CVsq and skewness-------------------------------

pdf("./Results/hays_results/skewness_results/hays_pearson_preserving_results_cvsq_skw_plots.pdf")
op<-par(mfrow=c(2,1))

#--------------CVsq histogrm-------------------------------------
xlm<-range(ans$cvsq_real,ans$cvsq_indep,stability_CP_hays$cvsq_surrogs)

hist(stability_CP_hays$cvsq_surrogs,col="grey",border=F,breaks=100,xaxt="n",xlim=xlm,xlab="10000 PP Surrogs CVsq",main="")
axis(side=1, at=c(xlm[1],0,xlm[2]))
abline(v=ans$cvsq_real,col="red") # actual CVsq from real data

#95% quantiles
abline(v=ans$cvsq_ntd_0.025CI,col="blue") 
abline(v=ans$cvsq_ntd_0.975CI,col="blue")

#50% quantiles
CI_cvsq_50<-quantile(stability_CP_hays$cvsq_surrogs,probs=c(.25,.75))
abline(v=CI_cvsq_50[1],col="green")
abline(v=CI_cvsq_50[2],col="green")

# Cvsq with no tail dep.
abline(v=ans$cvsq_ntd_median,col="magenta")

# Cvsq if indep.
abline(v=ans$cvsq_indep,col="black")


#--------------skw histogrm-------------------------------------
xlm<-range(ans$skw_real,ans$skw_indep,stability_CP_hays$skw_surrogs)

hist(stability_CP_hays$skw_surrogs,col="grey",border=F,breaks=100,xaxt="n",xlim=xlm,xlab="10000 PP Surrogs Skewness",main="")
axis(side=1, at=c(xlm[1],0,xlm[2]))
abline(v=ans$skw_real,col="red") # actual CVsq from real data

#95% quantiles
abline(v=ans$skw_ntd_0.025CI,col="blue") 
abline(v=ans$skw_ntd_0.975CI,col="blue")

#50% quantiles
CI_skw_50<-quantile(stability_CP_hays$skw_surrogs,probs=c(.25,.75))
abline(v=CI_skw_50[1],col="green")
abline(v=CI_skw_50[2],col="green")

# Skewness with no tail dep.
abline(v=ans$skw_ntd_median,col="magenta")

# Skewness if indep.
abline(v=ans$skw_indep,col="black")

# add legend
legend("topright",col=c("red","magenta","blue","green","black"),lty=c(1,1,1,1),
       horiz = F, bty="n",
       legend=c("real value","no Tail-dep. (median)","95%CI","50%CI","Indep."))

par(op)
dev.off()
```

<!--some additional fig for suppmat : hays-->
```{r hays_spaceavg_some_figs,echo=F,results="hide"}
source("vivj_matrix.R")

#UT dep. among all dry seasoned deep rooted grasses
pdf("./Results/hays_results/hays_spaceavg_ANGE_PAVI2_PSTE5.pdf",height=3,width=10)
op<-par(mfrow=c(1,3),mar=c(5,5,2,2))
i<-2
j<-11
d_allsp<-hays_spaceavg
s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=names(d_allsp[[1]])[j],cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.6,col="blue")
mtext(paste0("(sp_x, sp_y) = (",i," , ",j,")"),
          side = 3, line=0.15, adj=0.5, col="black")

i<-2
j<-13
d_allsp<-hays_spaceavg
s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=names(d_allsp[[1]])[j],cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.6,col="blue")
mtext(paste0("(sp_x, sp_y) = (",i," , ",j,")"),
          side = 3, line=0.15, adj=0.5, col="black")

i<-11
j<-13
d_allsp<-hays_spaceavg
s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=names(d_allsp[[1]])[j],cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.6,col="blue")
mtext(paste0("(sp_x, sp_y) = (",i," , ",j,")"),
          side = 3, line=0.15, adj=0.5, col="black")

par(op)
dev.off()


pdf("./Results/hays_results/hays_spaceavg_SCSC_interactions.pdf",height=10,width=6)
op<-par(mfrow=c(4,2),mar=c(4,5,3,3))
i<-5
j<-14
d_allsp<-hays_spaceavg

vivj_matrix(d_allsp = d_allsp,loc=1,i=i,j=j,ploton=T,onbounds=F,lb=NA,ub=NA)

s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=paste("-",names(d_allsp[[1]])[j],sep=""),cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.5,col="blue")

i<-12
j<-14
d_allsp<-hays_spaceavg

vivj_matrix(d_allsp = d_allsp,loc=1,i=i,j=j,ploton=T,onbounds=F,lb=NA,ub=NA)

s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=paste("-",names(d_allsp[[1]])[j],sep=""),cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.5,col="blue")

i<-17
j<-14
d_allsp<-hays_spaceavg

vivj_matrix(d_allsp = d_allsp,loc=1,i=i,j=j,ploton=T,onbounds=F,lb=NA,ub=NA)

s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=paste("-",names(d_allsp[[1]])[j],sep=""),cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.5,col="blue")

i<-18
j<-14
d_allsp<-hays_spaceavg

vivj_matrix(d_allsp = d_allsp,loc=1,i=i,j=j,ploton=T,onbounds=F,lb=NA,ub=NA)

s1<-vivj_matrix(d_allsp=d_allsp,loc=1,i=i,j=j,ploton=F,onbounds=F,lb=NA,ub=NA)
plot(s1$mat[,1],s1$mat[,2],col=rgb(0,0,0,0),pch=19,xlab=names(d_allsp[[1]])[i],
     ylab=paste("-",names(d_allsp[[1]])[j],sep=""),cex.lab=1.5)
text(s1$mat[,1],s1$mat[,2],labels = c(1932:1972),cex=0.5,col="blue")

par(op)
dev.off()
```

<!-- preparing spatial avg data in usual format with common+pseudo sp. for knz data-->
```{r read_and_prepare_knz_data,echo=F,results="hide"}
set.seed(seed)

dat<-read.csv("./Data/KnzData/Grassland_group1_master.csv")
dat_sitewise<-split(dat,dat$site)

dat_knz<-dat_sitewise$knz

# convert factor class into character class
dat_knz$site<-as.character(dat_knz$site)
dat_knz$plot<-as.character(dat_knz$plot)
dat_knz$subplot<-as.character(dat_knz$subplot)
dat_knz$uniqueID<-as.character(dat_knz$uniqueID)
dat_knz$unitAbund<-as.character(dat_knz$unitAbund)
dat_knz$scaleAbund<-as.character(dat_knz$scaleAbund)
dat_knz$species<-as.character(dat_knz$species)

# unique years?
uyr<-sort(unique(dat_knz$year))
lenyr<-length(uyr)

# unique plots ? 
uplot<-sort(unique(dat_knz$plot))
uplot
length(uplot)

# unique subplots ?
usplot<-sort(unique(dat_knz$subplot))
usplot
length(usplot)

# unique species ?
usp<-sort(unique(dat_knz$species))
usp<-as.character(usp)
lenusp<-length(usp)

#--------creating results folder for knz data--------------------
if(!dir.exists("./Results/knz_results")){
  dir.create("./Results/knz_results/")
}

if(!dir.exists("./Results/knz_results/skewness_results")){
  dir.create("./Results/knz_results/skewness_results/")
}

# Fill the following table with average cover from all plots for a given year with a given sp.
ts_all_sp_knz<-matrix(NA,nrow=lenyr,ncol=lenusp) # timeseries for each sp along each column
colnames(ts_all_sp_knz)<-usp
rownames(ts_all_sp_knz)<-uyr
ts_all_sp_knz<-as.data.frame(ts_all_sp_knz)

my_knz<-dat_knz[,c("year","species","abundance")]
my_knz_eachsp<-split(my_knz,my_knz$species)

all(names(my_knz_eachsp)==colnames(ts_all_sp_knz))==T # It's a check

for(i in 1:length(my_knz_eachsp)){
  x1<-my_knz_eachsp[[i]]
  ts_all_sp_knz[,i]<-sapply(split(x=x1$abundance,f=x1$year), mean)
}

#-------saving a big matrix 33yrs by 123 sp. which has each sp timeseries along each column : knz data-------
saveRDS(ts_all_sp_knz,"./Results/knz_results/ts_all_sp_knz.RDS")

anyNA(ts_all_sp_knz)

# screen sp. for KNZ
nyr_0_eachsp<-apply(ts_all_sp_knz,MARGIN = 2,FUN = function(x){sum(x==0)})
nyr_0_eachsp<-as.data.frame(nyr_0_eachsp)

# common sp. for KNZ : these sp present for all 33 years
id_common_sp_knz<-which(nyr_0_eachsp$nyr_0_eachsp==0) 
ts_common_knz<-ts_all_sp_knz[,id_common_sp_knz]

# rare sp. for KNZ
id_rare_sp_knz<-which(nyr_0_eachsp$nyr_0_eachsp>=31)
ts_rare_knz<-ts_all_sp_knz[,id_rare_sp_knz]

# normal or intermediate sp. for KNZ
id_interm_sp_knz<-which(nyr_0_eachsp$nyr_0_eachsp>0 & nyr_0_eachsp$nyr_0_eachsp<31)
ts_normal_knz<-ts_all_sp_knz[,id_interm_sp_knz]

# sp. category for KNZ
sp_category_knz<-data.frame(sp=rownames(nyr_0_eachsp),category=NA)
sp_category_knz$category[id_common_sp_knz]<-"C"
sp_category_knz$category[id_interm_sp_knz]<-"I"
sp_category_knz$category[id_rare_sp_knz]<-"R"

#---------saving a 123sp by 2 matrix indicating C/I/R category for each knz-sp---------
saveRDS(sp_category_knz,"./Results/knz_results/all_sp_category_spatialavg_knz.RDS")

# Now make the usual format for KNZ data (with common sp and all other merged into a pseudo species) to be used in model selection later

knz_spaceavg<-vector("list",1)
names(knz_spaceavg)<-"avg.percent.cover"

sp.screened.data<-vector("list",length(id_common_sp_knz))
names(sp.screened.data)<-rownames(nyr_0_eachsp)[id_common_sp_knz]

for(isp in 1:length(id_common_sp_knz)){
  sp.screened.data[[isp]]<-data.frame(Year=c(1983:2015),Dat=ts_common_knz[,isp])
}

knz_spaceavg$avg.percent.cover<-sp.screened.data

# Append the pseudo species = merged sp. of I & R category
pseudo_knz_IR<-apply(X=ts_all_sp_knz[,which(sp_category_knz$category%in%c("I","R"))],MARGIN = 1,FUN = sum)
pseudo_knz<-data.frame(Year=c(1983:2015),Dat=pseudo_knz_IR)
pseudo_knz<-list(pseudo_knz)

knz_spaceavg$avg.percent.cover<-append(knz_spaceavg$avg.percent.cover,pseudo_knz)
names(knz_spaceavg$avg.percent.cover)[[length(id_common_sp_knz)+1]]<-"pseudo_knz"

#---------saving the spatial avg. data for knz with whigh we will do taildep. analysis later----------------
saveRDS(knz_spaceavg,"./Results/knz_results/knz_spaceavg_data_CP.RDS")

#-----------saving a matrix with timeseries of common + 1 pseudo (all other merged into 1) sp. for knz------
ts_mat_CP_knz<-cbind(ts_common_knz,pseudo_knz_IR)
saveRDS(ts_mat_CP_knz,"./Results/knz_results/skewness_results/ts_mat_CP_knz.RDS") 

#--------------- time series plot for knz Common sp, Common + Normal(or Intermediate) sp., Common+Normal+Rare sp.-------------------

pdf("./Results/knz_results/skewness_results/total_timeseries.pdf",height=6,width=6)
op<-par(mar=c(5.1, 5.1, 1.1, 2.1))

total_ts_C<-apply(X=ts_all_sp_knz[,id_common_sp_knz],MARGIN = 1,FUN = sum)
total_ts_CI<-apply(X=ts_all_sp_knz[,sort(c(id_common_sp_knz,id_interm_sp_knz))],MARGIN = 1,FUN = sum)
total_ts_CIR<-apply(X=ts_all_sp_knz[,sort(c(id_common_sp_knz,id_interm_sp_knz,id_rare_sp_knz))],MARGIN = 1,FUN = sum)
total_ts<-cbind(total_ts_C,total_ts_CI,total_ts_CIR)
plot(c(1983:2015),total_ts[,1],ylim=range(total_ts),col=rgb(1,0,0,0.5),type="b",pch=16,xlab="Years",ylab="Total percent cover",xlim=c(1983,2015),cex.lab=2,cex.axis=2)
lines(c(1983:2015),total_ts[,2],type="b",pch=16,col=rgb(0,1,0,0.5))
lines(c(1983:2015),total_ts[,3],type="b",col="black",pch=1)
legend("topright",c("common sp.","common + intermediate sp.","all sp. including rare"),lty=c(1,1,1),
       col=c(rgb(1,0,0,0.5),rgb(0,1,0,0.5),"black"),pch=c(16,16,1),bty="n",cex=1.2)
par(op)
dev.off()

#------plot knz_spaceavg data for all common sp for all yearspan-------------------------------
good_sp<-c(1:length(knz_spaceavg[[1]]))
lensp<-length(good_sp)

summary_knz_commonsp<-data.frame(sp=names(knz_spaceavg$avg.percent.cover),n0=NA,nTies=NA)
pdf("./Results/knz_results/rawplot_knz_spaceavg_commonsp_with_pseudosp_avgcover.pdf",height = 0.5*lensp,width=0.5*lensp)
op<-par(mfrow=c(6,5),mar=c(5,5,3,3))
for (i in good_sp){
  n0<-sum(knz_spaceavg$avg.percent.cover[[i]]$Dat==0)
  nTies<-sum(duplicated(knz_spaceavg$avg.percent.cover[[i]]$Dat)==T)
  summary_knz_commonsp$n0[i]<-n0
  summary_knz_commonsp$nTies[i]<-nTies
  if(n0==0){
    col1<-rgb(1,0,0,0.3) # these are the sp. present for all years : total 27 sp. + 1 pseudo sp.
  }else{
    col1<-rgb(0,0,1,0.3)
  }
  plot(knz_spaceavg$avg.percent.cover[[i]]$Year,knz_spaceavg$avg.percent.cover[[i]]$Dat,col=col1,pch=19,
       ylim=c(0,max(knz_spaceavg$avg.percent.cover[[i]]$Dat)),xlab="Year (1983-2015)",type="b",ylab="avg. % cover")
  abline(h=0)
  
  mtext(paste0("sp = ",i," : ",names(knz_spaceavg$avg.percent.cover)[i]," ,nT=",nTies,sep=""))
}
par(op)
dev.off()

# ---------------------generate copula plots for all common sp for knz_spaceavg--------------------
source("./vivj_matrix.R")

pdf("./Results/knz_results/copulaplot_knz_spaceavg_commonsp_with_pseudosp_avgcover.pdf",height=2*lensp,width = 2*lensp)
op<-par(mfrow=c(lensp,lensp),mar=c(3,3,3,3), mgp=c(1.5,0.5,0))
for(i in c(1:lensp)){
  for(j in c(1:lensp)){
    vivj_matrix(d_allsp=knz_spaceavg,loc=1,
                i=good_sp[i],j=good_sp[j],level=0.05,
                ploton=T,onbounds=T,lb=0,ub=0.5)
  }
}
par(op)
dev.off()

# !!!!!!!!!!!!!!!!!!!!!! NOTE : even I consider the species which were present every year still there was large nTies with sp. 5,7,20
# But there were very few weird copula with those sp. or max. independent : that's why I don't care much
```

<!-- non-parametric analysis for cor stat with common + pseudo sp. in knz-->
```{r npa_knz_spaceavg, echo=F, cache=T, cache.extra=list(seed,knz_spaceavg,mtime("NonParamStat.R"),mtime("vivj_matrix.R"),mtime("CopulaFunctions.R"), mtime("CopulaFunctions_flexible.R"))}

set.seed(seed) 
source("./NonParamStat.R")

if(!dir.exists("./Results/knz_results/corstat_knz_spaceavg_results")){
  dir.create("./Results/knz_results/corstat_knz_spaceavg_results")
}

resloc<-"./Results/knz_results/corstat_knz_spaceavg_results/"

corstat_knz_spaceavg<-multcall(d_allsp=knz_spaceavg,
                           loc=1,
                           resloc=resloc,
                           good_sp=c(1:length(knz_spaceavg[[1]])),
                           nbin=2)

saveRDS(corstat_knz_spaceavg,paste(resloc,file="corstat_knz_spaceavg.RDS",sep=''))
```

<!-- genarating Corl - Coru plots from non-parametric analysis with common + pseudo sp. in knz-->
```{r plot_res_knz_npa_spaceavg, echo=F, results="hide",cache=T,warning=F, cache.extra=list(seed,corstat_knz_spaceavg,mtime("NonParamStat_matrixplot.R"),mtime("mycorrplot.R"))}
set.seed(seed)
source("./NonParamStat_matrixplot.R")

resloc<-"./Results/knz_results/corstat_knz_spaceavg_results/"

CorlmCoru_knz_spaceavg<-NonParamStat_matrixplot(data=corstat_knz_spaceavg,resloc=resloc,tagon=T,type="lower",wd=13,ht=13)
saveRDS(CorlmCoru_knz_spaceavg,paste(resloc,file="CorlmCoru_knz_spaceavg.RDS",sep=''))
```

<!--get appropriate surrogates for knz with common+pseudo sp.-->
```{r make_surrogs_CP_knz, echo=F, results="hide", warning=F, cache=T, cache.extra=list(surrog_seed1,surrog_seed2,ts_mat_CP_knz,mtime("copmap.R"),mtime("getinv.R"),mtime("getinvrg.R"),mtime("alignranks.R"))}

#***setup for the chunk

library(matrixcalc)
library(mvtnorm)

# functions needed
source("copmap.R")
source("getinv.R")
source("getinvrg.R")
source("alignranks.R")

# result folder to save surrogates 
resloc_surrog_knz<-"./Results/knz_results/skewness_results/pp_surrogs_knz_CP/"
if (!dir.exists(resloc_surrog_knz)){
  dir.create(resloc_surrog_knz)
}

#***For each pair of species, find the preimage of the correlation 
#under the map defined by copmap.R - this is to determine search bounds
#for the subsequent optimization

set.seed(surrog_seed1)

d<-ts_mat_CP_knz
numsp<-dim(d)[2]
p<-seq(from=-1,to=1,by=0.1)
numreps<-250

#receptacle for results, the preimage in allparms[,,2], the range
#specified by the other values
allparms<-array(NA,c(numsp,numsp,3))

#loop through each pair of species
for (c1 in 1:(numsp-1))
{
  print(paste0("c1: ",c1))
  for (c2 in (c1+1):numsp)
  {
    #get the time series for these two species
    x<-d[,c1]
    y<-d[,c2]
    
    #get the (stochastic) map
    thisres<-copmap(x,y,p,numreps,"cor")
    
    #find the iverse range
    allparms[c1,c2,c(1,3)]<-getinvrg(p,thisres,cor(x,y))
    
    #find the actual preimage
    allparms[c1,c2,2]<-getinv(p,thisres,cor(x,y),"median")
    
    #make and save a plot
    mn<-apply(FUN=mean,MARGIN=2,X=thisres)
    quants<-apply(FUN=quantile,MARGIN=2,X=thisres,probs=c(.25,.5,.75))
    pdf(paste0(resloc_surrog_knz,"mapplot_",c1,"_",c2,".pdf"))
    plot(p,mn,type='l',ylim=range(mn,quants))
    lines(p,quants[2,],type='l',lty="dashed")
    lines(p,quants[1,],type='l',lty="dotted")
    lines(p,quants[3,],type='l',lty="dotted")
    lines(range(p),rep(cor(x,y),2))
    lines(rep(allparms[c1,c2,1],2),c(-1,1))
    lines(rep(allparms[c1,c2,2],2),c(-1,1))
    lines(rep(allparms[c1,c2,3],2),c(-1,1))
    dev.off()
  }
}

#make sure the preimage is within the preimage range in all cases,
#and you got a sensible range and a single-point preimage in all cases
if (sum(allparms[,,1]<allparms[,,2],na.rm=TRUE)!=(numsp^2-numsp)/2 ||
    sum(allparms[,,2]<allparms[,,3],na.rm=TRUE)!=(numsp^2-numsp)/2)
{
  stop("Error in make_surrogs_CP_knz: error screening location 1")
}

#***Now look for positive definite in a couple simple ways

#First check to see if we got lucky and the preimage itself is pos def
m1<-allparms[,,2]
m1[is.na(m1)]<-0
m1<-m1+t(m1)
diag(m1)<-1
if (is.positive.semi.definite(m1))
{
  stop("We got lucky in make_surrogs_CP_knz, revisit code: lucky screening location 1") 
} #If m1 is pos semi def, we can skip much of the rest of the chunk and
#do things in a simpler way, so screen for that

#now use nearPD and see if you can get a positive definite matrix between the bounds
hlo<-P2p(t(allparms[,,1]))
hpre<-P2p(t(allparms[,,2]))
hhi<-P2p(t(allparms[,,3]))
hmid<-(hlo+hhi)/2
bestmatNPD<-matrix(as.numeric(Matrix::nearPD(m1,corr=TRUE)$mat),numsp,numsp)
bestmatNPDvect<-P2p(bestmatNPD)
if (sum(bestmatNPDvect>hhi | bestmatNPDvect<hlo)==0)
{
  stop("We got lucky in make_surrogs_CP_knz, revisit code: lucky screening location 2") 
} #if we got a positive definite matrix here, then we may not have to search
#for one below, so screen for that

#***Now search for a positive definite matrix in the space of parameters 
#desribed by allparms

#prepare
set.seed(surrog_seed2)
numpd<-50000
allposdefmats<-matrix(NA,2*numpd,length(hlo)+1)
allposdefmats_rowcount<-1

#the function to optimize which will lead us toward pos def mats, and will also
#record any that are found
fn<-function(h)
{
  res<-min(eigen(p2P(h))$values)
  if (res>0)
  {
    allposdefmats[allposdefmats_rowcount,]<<-c(h,res)
    allposdefmats_rowcount<<-allposdefmats_rowcount+1 #note the use of global variables, watch out!
    print("Found one!")
  }
  return(res)
}

#do a search starting with the preimage itself
optim(hpre,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
      control=list(fnscale=-1)) 

#do a search starting from the midpoint of the preimage range
optim(hmid,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
      control=list(fnscale=-1)) 

#now do a bunch of searches from random start points
while (allposdefmats_rowcount<=numpd)
{
  startvec<-(hhi-hlo)*runif(length(hlo))+hlo
  optim(startvec,fn,method="L-BFGS-B",lower=hlo,upper=hhi,
        control=list(fnscale=-1)) 
}

allposdefmats<-allposdefmats[1:(allposdefmats_rowcount-1),]
saveRDS(allposdefmats,file=paste0(resloc_surrog_knz,"PosDefMats.RDS"))

#***Now find the result that is closest to hpre

allposdefmats<-allposdefmats[,1:(dim(allposdefmats)[2]-1)]
hpremat<-matrix(hpre,dim(allposdefmats)[1],length(hpre),byrow = TRUE)
sizemat<-matrix((hhi-hlo)/2,dim(allposdefmats)[1],length(hlo),byrow = TRUE)
dist<-apply(X=((allposdefmats-hpremat)/sizemat)^2,FUN=sum,MARGIN=1)
besth<-allposdefmats[which(dist==min(dist)),]
if (any(besth<hlo | besth>hhi))
{
  stop("Error in make_surrogs_CP_knz: error screening location 2")
}#the result should be within the bounds - screen for it
bestmat<-p2P(besth) #this is supposed to be my parameter matrix for a normal copula!
if (!is.positive.semi.definite(bestmat))
{
  stop("Error in make_surrogs_CP_knz: error screening location 3")
}#the result should be positive semi-definite - screen for it

#***Now make a plot showing where in the ranges given by hlo to hhi 
#the besth values sit

inds<-order(besth)
pdf(file=paste0(resloc_surrog_knz,"PDResultComparedToBounds.pdf"))
plot(1:length(besth),besth[inds],type='l',ylim=range(besth,hlo,hhi))
lines(1:length(besth),hlo[inds],type='l',lty="dashed")
lines(1:length(besth),hhi[inds],type='l',lty="dashed")
dev.off()

#***Now assess whether you get reasonably pearson-preserving surrogates
#using bestmat

numsimsforcheck<-1000

sims<-rmvnorm((dim(d)[1])*numsimsforcheck,sigma=bestmat)
sims<-aperm(array(sims,c(dim(d)[1],numsimsforcheck,dim(d)[2])),c(1,3,2))
dim(sims)
sd<-d
for (counter in 1:numsp)
{
  sd[,counter]<-sort(d[,counter])
}#need to sort the columns of d to use alignranks
remapd<-alignranks(sd,sims)
holder<-apply(FUN=cor,MARGIN=3,X=remapd)
allcors<-array(holder,c(numsp,numsp,numsimsforcheck))
gtres<-apply(FUN=sum,MARGIN=c(1,2),X=(allcors>array(cor(d),c(numsp,numsp,numsimsforcheck))))
diag(gtres)<-NA
pdf(file=paste0(resloc_surrog_knz,"AssessIfSurrogsPreservePearson.pdf"))
hist(gtres)#very few of these should be outside the range 250-750, 
#and if they are not outside that range you have pretty 
#good surrogates
dev.off()

saveRDS(bestmat,file=paste0(resloc_surrog_knz,"FinalPDParameterMatrix.RDS"))
#This is the final parameter result. If you generate data from a normal copula
#with this parameter matrix and then use alignranks, that should give ok 
#Pearson-preserving surrogates (though I still need to check if CV_com^2
#is preserved.

#***Now produce 100000 surrogates and save for later use

sims<-rmvnorm((dim(d)[1])*100000,sigma=bestmat)
sims<-aperm(array(sims,c(dim(d)[1],100000,dim(d)[2])),c(1,3,2))
surrogs<-alignranks(sd,sims)
saveRDS(surrogs,file=paste0(resloc_surrog_knz,"Surrogates.RDS"))

#***compute CV_com^2 for surrogs and compare to data value
totpop<-apply(FUN=sum,X=surrogs,MARGIN=c(1,3))
allvars<-apply(FUN=var,X=totpop,MARGIN=2)
allmnsq<-apply(FUN=function(x){(mean(x))^2},X=totpop,MARGIN=2)
allCVcomsq<-allvars/allmnsq
totpop<-apply(FUN=sum,X=d,MARGIN=1)
CVd<-var(totpop)/((mean(totpop))^2)
pdf(paste0(resloc_surrog_knz,"HistogramPlotForCVcomsq.pdf"))
hist(allCVcomsq,50)
lines(rep(CVd,2),c(1,1000),col="red")
dev.off()
saveRDS(sum(allCVcomsq<CVd)/length(allCVcomsq),
        paste0(resloc_surrog_knz,"FracSurrogCVcomsqLTactual.RDS"))

surrogs_CP_knz<-surrogs  # this is the pearson preserving surrogs for 
#use in subsequent chunks
```

<!-- genarating stability based results and plots for knz-->
```{r skewness_knz_spaceavg_PP, echo=F, results="hide", warning=F, cache=T, cache.extra=list(seed,ts_mat_CP_knz,surrogs_CP_knz,mtime("make_tab_stability_assessment.R"),mtime("mycvsq.R"),mtime("SkewnessAnd3CentMom.R"))}

set.seed(seed)
source("make_tab_stability_assessment.R")

# randomly sample numsurrog surrogate matrices from Pearson preserving array
numsurrog<-10000
id_surrogs<-sample(c(1:dim(surrogs_CP_knz)[3]),numsurrog,replace=F)
surrogs_CP_knz<-surrogs_CP_knz[,,id_surrogs]

stability_CP_knz<-make_tab_stability(m=ts_mat_CP_knz,surrogs = surrogs_CP_knz, surrogs_given = T)
ans<-(stability_CP_knz$df_stability)
rownames(ans)<-"C+P"
class(ans)

write.csv(ans,"./Results/knz_results/skewness_results/knz_stability_CP.csv")

#--------------generate plots with knz stability results : CVsq and skewness-------------------------------

pdf("./Results/knz_results/skewness_results/knz_pearson_preserving_results_cvsq_skw_plots.pdf")
op<-par(mfrow=c(2,1))

#--------------CVsq histogrm-------------------------------------
xlm<-range(ans$cvsq_real,ans$cvsq_indep,stability_CP_knz$cvsq_surrogs)

hist(stability_CP_knz$cvsq_surrogs,col="grey",border=F,breaks=100,xaxt="n",xlim=xlm,xlab="10000 PP Surrogs CVsq",main="")
axis(side=1, at=c(xlm[1],0,xlm[2]))
abline(v=ans$cvsq_real,col="red") # actual CVsq from real data

#95% quantiles
abline(v=ans$cvsq_ntd_0.025CI,col="blue") 
abline(v=ans$cvsq_ntd_0.975CI,col="blue")

#50% quantiles
CI_cvsq_50<-quantile(stability_CP_knz$cvsq_surrogs,probs=c(.25,.75))
abline(v=CI_cvsq_50[1],col="green")
abline(v=CI_cvsq_50[2],col="green")

# Cvsq with no tail dep.
abline(v=ans$cvsq_ntd_median,col="magenta")

# Cvsq if indep.
abline(v=ans$cvsq_indep,col="black")

# add legend
legend("topright",col=c("red","magenta","blue","green","black"),lty=c(1,1,1,1),
       horiz = F, bty="n",
       legend=c("real value","no Tail-dep. (median)","95%CI","50%CI","Indep."))

#--------------skw histogrm-------------------------------------
xlm<-range(ans$skw_real,ans$skw_indep,stability_CP_knz$skw_surrogs)

hist(stability_CP_knz$skw_surrogs,col="grey",border=F,breaks=100,xaxt="n",xlim=xlm,xlab="10000 PP Surrogs Skewness",main="")
axis(side=1, at=c(xlm[1],0,xlm[2]))
abline(v=ans$skw_real,col="red") # actual CVsq from real data

#95% quantiles
abline(v=ans$skw_ntd_0.025CI,col="blue") 
abline(v=ans$skw_ntd_0.975CI,col="blue")

#50% quantiles
CI_skw_50<-quantile(stability_CP_knz$skw_surrogs,probs=c(.25,.75))
abline(v=CI_skw_50[1],col="green")
abline(v=CI_skw_50[2],col="green")

# Skewness with no tail dep.
abline(v=ans$skw_ntd_median,col="magenta")

# Skewness if indep.
abline(v=ans$skw_indep,col="black")

par(op)
dev.off()
```

```{r pedagog_fig, echo=F, results="hide", warning=F, cache=T, cache.extra=list(seed,mtime("ExtremeTailDep.R"),mtime("ncsurrog.R"))}
library(copula)
library(e1071)
source("./ncsurrog.R")
source("./ExtremeTailDep.R")

set.seed(seed)
nsp<-25
nyr<-500

#----------------calling copula-----------------
#par<-5
#cop<-claytonCopula(param=par,dim=nsp)
#cop<-gumbelCopula(param=par,dim=nsp)
#cmat<-rCopula(n=nyr,cop)
#plot(cmat[,1],cmat[,2]) # LTdep
#----------------------------------------------

#------------------calling extreme copula for LTdep.---------------
xmat<-retd(n=nyr,d=nsp,rl=-1,mn=0,sdev=1) #LTdep.
range(xmat)
cmat<-xmat+5 # this line makes cover data positive
any(cmat<0) # This should be FALSE
#plot(xmat[,1],xmat[,2])
#plot(cmat[,1],cmat[,2])
#plot(c(1:nyr),cmat[,1])
tot_ts_l<-apply(cmat,MARGIN = 1, FUN=sum)

surrog_array<-ncsurrog(m=cmat,corpres="spearman",numsurrog=1,plotcheckon=F,resloc="./Results")
surrog_mat<-surrog_array[,,1]
#plot(surrog_mat[,1],surrog_mat[,2])
tot_ts_surrogl<-apply(surrog_mat,MARGIN = 1, FUN=sum)

skw_real_l<-skewness(tot_ts_l,type=2)
skw_surrogl<-skewness(tot_ts_surrogl,type=2)

#------------------calling extreme copula for RTdep.---------------
xmat<-retd(n=nyr,d=nsp,rl=1,mn=0,sdev=1) #RTdep.
range(xmat)
cmat<-xmat+5 # this line makes cover data positive
any(cmat<0) # This should be FALSE
#plot(cmat[,1]+10,cmat[,2]+10)
#plot(c(1:nyr),cmat[,1]+10)

tot_ts_r<-apply(cmat,MARGIN = 1, FUN=sum)

surrog_array<-ncsurrog(m=cmat,corpres="spearman",numsurrog=1,plotcheckon=F,resloc="./Results")
surrog_mat<-surrog_array[,,1]
tot_ts_surrogr<-apply(surrog_mat,MARGIN = 1, FUN=sum)

skw_real_r<-skewness(tot_ts_r,type=2)
skw_surrogr<-skewness(tot_ts_surrogr,type=2)

#---------------Now plot-----------------------------------------
ylm<-range(tot_ts_l,tot_ts_surrogl,tot_ts_surrogr)
pdf("./Results/pedagog_figs/extremeLT_tot_ts.pdf",height=2,width=6)
op<-par(mar=c(4.1, 4.1, 0.5, 1.1))
plot(c(1:nyr),tot_ts_surrogl,type="l",ylim=c(ylm[1],ylm[2]+50),col="grey",xlab="Years",ylab="Total biomass")
lines(c(1:nyr),tot_ts_l)
legend("topleft",c(paste("LTdep., skew=",round(skw_real_l,3),sep=""),paste("No Tdep., skew=",round(skw_surrogl,3),sep="")),lty=c(1,1),horiz=T,
       col=c("black","grey"),bty="n",cex=1)
par(op)
dev.off()

pdf("./Results/pedagog_figs/extremeRT_tot_ts.pdf",height=2,width=6)
op<-par(mar=c(4.1, 4.1, 0.5, 1.1))
plot(c(1:nyr),tot_ts_surrogr,type="l",ylim=c(ylm[1],ylm[2]+50),col="grey",xlab="Years",ylab="Total biomass")
lines(c(1:nyr),tot_ts_r)
legend("topleft",c(paste("RTdep., skew=",round(skw_real_r,3),sep=""),paste("No Tdep., skew=",round(skw_surrogr,3),sep="")),lty=c(1,1),horiz=T,
       col=c("black","grey"),bty="n",cex=1)
par(op)
dev.off()
#------------------------------------------------------------------
# another pedagog fig. showing LT, UT with same correlation
library(VineCopula)
par_rho_0.8<-copula::iRho(claytonCopula(5),rho=0.8)
ccop<-BiCopSim(500,3,par=par_rho_0.8)
sccop<-BiCopSim(500,13,par=par_rho_0.8)
pdf("./Results/pedagog_figs/LTUT_rho_0.8.pdf",height=2,width=4)
op<-par(mfrow=c(1,2),mar=c(2.5, 2.5, 0.5, 1.1),mgp=c(1.3,0.5,0))
plot(ccop[,1],ccop[,2],col=rgb(0,0,0,0.1),pch=20,xlab="u",ylab="v",cex.lab=1,cex.axis=0.8)
plot(sccop[,1],sccop[,2],col=rgb(0,0,0,0.1),pch=20,xlab="u",ylab="v",cex.lab=1,cex.axis=0.8)
par(op)
dev.off()
```

# Methods to generate extreme asymmetric tail dependence in a community matrix \label{extreme_taildep_mat} 

\noindent Random numbers were generated by first producing a $n$ by $d$ matrix, $m$, of 
independent standard-normal random draws, where $n$ is the length of time series and $d$
is the number of species in the community. Then, each row of $m$ 
was identified for a first set of modifications randomly 
and independently with a $50\%$ chance; 
for those rows identified, every element of the row was replaced with the 
absolute value of the first element of the row (including the first element itself).
For other rows, every element in the row was replaced by the negative of its
own absolute value. To make the matrix $m$ having non-negative numbers (as they correspond
to species-biomass), we gave a positive shift to all the entries in $m$. This produces 
right-tail dependent noise, normally distributed
for each location. If left-tail dependent noise is desired, take the negative of the
entire matrix.


# Resampling methods to generate correlation preserving Normal copula \label{skew_ncsurrog} 

<!-- This section is adapted from BIVAN----------->
For multivariate datasets (e.g. a matrix with biomass or basal cover timeseries for each plant species along each column),
surrogates were produced as follows:

- Given data $x_i(t)$ for $i=1,\ldots,N$ and $t=1,\ldots,T$, let $\tau_{ij}$ be the Spearman (or Kendall) 
correlation of $x_i(t)$ and $x_j(t)$.
- Calculate the covariance matrix of a multivariate normal distribution with standard-normal
marginals and with pairwise Spearman (or Kendall) correlations equal to the $\tau_{ij}$. This is possible
using the `iRho` function (or the `iTau` function) of the `copula` package. 
- Generate data $a_i(t)$ for $i=1,\ldots,N$ and $t=1,\ldots,T$ by taking $T$ independent draws
from the the multivariate normal distribution of the previous step.
- For any $i,t$ for which the datum $x_i(t)$ was missing, replace $a_i(t)$ by `NA`.
- For each $k$ and $i$ for which $a_i(t)$ is not `NA`, replace the 
$k$th-smallest (non-`NA`) element of the time series 
$a_i(t)$, $t=1,\ldots,T$ by the $k$th smallest (non-`NA`) 
element of $x_i(t)$, $t=1,\ldots,T$.
Call the result $b_i(t)$. This is a surrogate dataset.

\noindent Our code for this algorithm is the function [`ncsurrog`](https://github.com/sghosh89/CSS/blob/master/ncsurrog.R).
The code for the Kendall version of the
algorithm throws an error if there are ties in any of the time series. This is
because the Kendall correlation in the very commonly used `cor` function in R 
is the Kendall tau-b coefficient in the event of ties, and index which 
differs in its definition from the commonly used tau-a that works when there are
no ties; and `tau` and `iTau` are intended for continuous copulas, which cannot 
produce data with ties. Our code also throws an error in the event that
the matrix generated in the second step is not positive semidefinite. Because
ties are common in our data, we only used the Spearman case in analyses using `ncsurrog`,
though, for expansibility, the `ncsurrog` code itself is written to handle 
the Kendall case as well when there are no ties.
 
Because, for each $i$, the final time series 
$b_i(t)$ is a permuted version of $x_i(t)$ (and with `NA`s for the same $t$), 
the marginal distributions of the surrogate dataset (i.e., the distributions of 
values for each sampling location, $i$) are exactly the same as for the original data.
Because ranks are preserved in passing from $a_i(t)$ to $b_i(t)$, pairwise Spearman 
(or Kendall) correlations for the $b_i(t)$ will be the same as those of the $a_i(t)$. 
Discrepancies between the $\tau_{ij}$ and the pairwise Spearman (or Kendall) 
correlations of the $b_i(t)$ will therefore come about only from the presence of NAs
and from sampling variation. 
These discrepancies should be minor if the 
numbers of NAs are not too large and $T$ is not too small. 

If the covariance matrix is not positive semi-definite, surrogates $b_i(t)$ are generated with nearest 
positive semidefinite covariance matrix and our code throws a warning. A histogram is made with the probabilities 
of each pairwise correlations (Spearman or Kendall) of all $b_i(t)$ greater than 
the corresponding pairwise correlations (Spearman or Kendall) of $a_i(t)$. 
For an acceptable nearest positive semidefinite covariance matrix, this histogram 
should spread around 50\% probability with no extreme tail.

# Data \label{Data} 

<!--Hays data-->
Every year from 1932-1972, researchers from Fort Hays State University, 
in Hays, Kansas, mapped all individual plants in a series 
of 1 sq. meter quadrats in a mixed grass prairie (38.8$^\circ$N, 99.3$^\circ$W) 
[@albertson1937ecology; @albertson1965vegation]. In fact, data produced 
using the pantograph technique can be used the same way as data generated by tagging
individual plants, with spatial coordinates substituting for tags. 
The Hays data set later had been digitized [@adler2007long] and is now available
[online](http://esapubs.org/archive/ecol/E088/161/). $41$ years (1932-1972)
long spatio-temporal data was widely used in earlier studies to understand plant demographic process. 
Here we will give a brief description of the [data](http://esapubs.org/archive/ecol/E088/161/).
`allrecords.csv` is basal cover (area in $cm^2$)
datasets for 151 unique species (when observed) recorded for 41 years from 51 (1m $\times$ 1m) quadrats.
Counts for each unique species occurred in the entire `allrecords.csv` dataset 
were stored in `species_list.csv` file with categories like `type`, `PLANTS.Symbol`,
`PLANTS.Synonym` (for explanation see [`metadata file`](http://esapubs.org/archive/ecol/E088/161/metadata.htm)). 
5 of 151 species were classified in `'remove'` category as mentioned in `type` column of `species_list.csv`. They were non-plant species
like `Bare ground`, `Fragment` and other unidentified sp. like `Mixed grass`, `Polygonum spp.` (only occurred once), `Unknown`.
For our purpose, we calculated average basal cover over the quadrats surveyed for each year for 
each 146 species (excluding the `remove` category from 151 species list). 

Among 51 quadrats, 36 permanent quadrats were arranged along a gradient of soil depth and located in two livestock exclosures
("e1" and "e2"). Rest 15 quadrats were located in grazed areas outside the exclosures, mostly in the 
shortgrass community. Location of 51 quadrats were recorded as (quadX $cm$, quadY $cm$)
in East-West and North-South direction, respectively relative to the south-west corner 
of exclosure "e1" (see `quadrat_info.csv`). Each quadrat have some polygons with ID number processed 
in GIS tracking position for individual plants. 
Each polygon location was recorded as centroid co-ordinate (x $cm$, y $cm$) of 
corresponding polygon (see `allrecords.csv`). 

<!--add description about KNZ data-->



<!--data category-->
For both datasets, we classified all the species into 3 distinct categories depending on their 
availability throughout the study period over all the quadrats considered. They are 
(i) "common sp." which were present at least 35 years for Hays from all over the plots, as Konza priarie 
data is bit shorter than Hays, we considered common sp. in this case should present for all 33 years (1983-2015).
(ii) "rare sp." which were present maximum for 2 years, (iii) the rests were
put into "intermediate" class. For tail dependence analysis,
we considered only the common sp. for each datasets (see 
Tables \ref{tab_spinfo_hays} and \ref{tab_spinfo_knz}) alng with the other species merged into 
a single pseudo species category. For skewness 
analysis we used all three categories along with some combinations of 
them to verify the consistency between different 
approaches (see Fig. \ref{fig_hays_spaceavg_skewness_cvsq}).


# Methods \label{Methods} 

<!--non-parametric analysis  : more or less similar with BIVAN paper-->
## Nonparametric approach \label{nonparam}

We used three nonparametric
statistics which can quantify the extent to which 
two normalized ranks $u_i$ and $v_i$ are 
related in any part of their distributions; we here describe the 
statistics. The statistics were formulated with positively associated
variables in mind. So definitions and associated 
diagrams will assume positive associations, though generalizations are 
straightforward. Given two bounds 
$0 \leq l_b < u_b \leq 1$, we define the parallel lines $u+v=2l_b$ and 
$u+v=2u_b$, which intersect the unit square (Fig. \ref{fig_stat_image}). 
Our statistics quantify the dependence
between $u_i$ and $v_i$ in the region bounded by these lines.
Using $l_b=0$ and some value $u_b\leq 0.5$ can give information about 
dependence in the left parts of the distributions of $u$ and $v$, and 
using $u_b=1$ and $l_b \geq 0.5$ can give information about dependence in 
the right parts of the distributions. 
The first statistic, denoted $\cor_{l_b,u_b}(u,v)$, is the portion of the Spearman 
correlation of $u_i$ and $v_i$ that is attributable to the points 
lying in the region given by the bounds $u+v=2l_b$ and $u+v=2u_b$:
\begin{equation}\label{eq.Cor}
   \cor_{l_b,u_b}(u,v) = \frac{\sum 
   (u_i-\mean(u)) (v_i-\mean(v))}{(n-1)\sqrt{\var(u)\var(v)}}.
\end{equation}
\noindent Here means and sample variances are computed using all $n$ data 
points, but the
sum is over only the indices $i$ for which $u_i+v_i > 2l_b$ and 
$u_i+v_i < 2u_b$. 
Larger values of $\cor_{l_b,u_b}$ indicate stronger positive association
between $u$ and $v$ in the region given by the bounds. The sum of 
$\cor_{0,0.5}(u,v)$ and $\cor_{0.5,1}(u,v)$ (or some other choice of 
$\cor_{l_{b_k},u_{b_k}}(u,v)$ for bounds $l_{b_k},u_{b_k}$ that partition the
interval $(0,1)$) equals the standard Spearman correlation. 

We also defined a statistic $\Ps_{l_b,u_b}$, 
which has a similar 
interpretation to $\cor_{l_{b_k},u_{b_k}}(u,v)$; we use both statistics 
because $\cor_{l_{b_k},u_{b_k}}(u,v)$ is more familiar, but 
in some instances $\Ps_{l_b,u_b}$ appears to have more power to
reveal tail dependence than does $\cor_{l_{b_k},u_{b_k}}(u,v)$. 
We define $\Ps_{l_b,u_b}$, using the bounds
$u+v=2l_b$ and $u+v=2u_b$, as follows. For a distance $h$, we define
$S(h)$ to be the number of points $(u_i,v_i)$ within the bounds 
and a distance less than $h$ from the line $v=u$, divided by the 
total number of points within the bounds. This function is
defined for $h$ going from $0$ to a distance $h_{\text{max}}$ which
is half the longer of the two segments obtained by intersecting the
bounds with the unit square. It is easy to see $S(0)=0$ and 
$S(h_{\text{max}})=1$. We define $S_i(h)$, also for $h$ going from $0$
to $h_{\text{max}}$, to be the area within the bounds and the unit square
and within distance $h$ of the line $v=u$, divided by the area
within the bounds and the unit square. This is the expected value of
$S(h)$ for independent data. We then define
\begin{equation}\label{eq.P}
   P_{l_b,u_b} = \int_0^{h_{\text{max}}} ( S(h) - S_i(h) ) dh.
\end{equation}
\noindent As for $\cor_{l_b,u_b}$, larger values of $\Ps_{l_b,u_b}$ 
indicate stronger positive association
between $u$ and $v$ in the region given by the bounds.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=6 cm,height=6 cm]{./stat_image.jpg} 
\caption[For definitions of non-parametric statistics]{Schematic diagram supporting definitions
of non-parametric tail dependence statistics. The blue and red lines are, 
respectively, $u+v=2l_b$ and $u+v=2u_b$.\label{fig_stat_image}}
\end{center}
\end{figure}

Our third statistic, $\Dsq_{l_b,u_b}(u,v)$, is the average squared 
distance between points satisfying $u_i+v_i>2l_b$ and 
$u_i+v_i<2u_b$ and the line $v=u$. Unlike $\cor_{l_b,u_b}$ and 
$\Ps_{l_b,u_b}$, which are large when points satisfying $u_i+v_i>2l_b$ and 
$u_i+v_i<2u_b$ cluster close to the line $v=u$, $\Dsq_{l_b,u_b}$ is 
small in that case. 
Thus large values of $\cor_{l_b,u_b}$ and $\Ps_{l_b,u_b}$ indicate strong 
dependence in the portions of the distributions of $u$ and $v$ given by 
the bounds, whereas small values of 
$\Dsq_{l_b,u_b}$ indicate strong dependence. 

For large data sets (large $n$),
we could use $l_b$ and $u_b$ close together without incurring undue
sampling variation in our statistics, and we could consider 
several different bands $(l_b,u_b)$ to understand how dependence
varies in different parts of the distributions. But for data sets with 
smaller $n$ we considered only $l_b=0$, $u_b=0.5$ and $l_b=0.5$, $u_b=1$ to see 
the dependence in lower and upper tail of the copula.
We use the shorthand $\cor_l=\cor_{0,0.5}$ ($l$ is for "lower") 
and $\cor_u=\cor_{0.5,1}$ ($u$ is for "upper"); likewise $\Ps_l=\Ps_{0,.5}$,
$\Ps_u=\Ps_{0.5,1}$, $\Dsq_l=\Dsq_{0,0.5}$, and $\Dsq_u=\Dsq_{0.5,1}$. 

To test for asymmetry of dependence in upper and lower portions of distributions,
we used differences of lower and upper statistics.
The difference $\cor_l-\cor_u$ is expected to be positive if 
dependence in the left halves of the distributions is stronger 
than dependence in their right halves; likewise for $\Ps_l-\Ps_u$.
The difference $\Dsq_u-\Dsq_l$ (note the opposite order), is also expected to be 
positive under the same conditions. These statistics were used for smaller data sets.

# Comparing results from two approaches \label{compare_methods} 

From model selection approach, we got results of more lower tail-dependence when we 
considered pairwise copula for most common species cover data 
for both data sets (Hays and Konza). To ensure our 
finding we also used alternative methods,
i. e., non-parametric analysis to implement 3 statistics 
($\cor_l -\cor_u$, $\Ps_l-\Ps_u$ and $\Dsq_u-\Dsq_l$).
Using each statistics we got similar interaction matrix 
(Figs. \ref{fig_hays_spaceavg_npa} and \ref{fig_knz_spaceavg_npa}) as of
Figs. \ref{MT-fig_LTmUT_spaceavg_hays}  and \ref{MT-fig_LTmUT_spaceavg_knz} 
in the main text, respectively, 
and counted the number of lower and upper tail dependent cells 
which were positively correlated. Result showed major 
lower tail dependence in 
Hays (i.e. $nL > nU$) and stronger upper 
tail dependence (i.e. $nU > nL$)
in Konza dataset. The mere discrepancy arises due to the fact for 
non-parametric analysis used a much wider boundary to see 
tail-dependence (due to smaller sample size)
whereas model selection result was based on extreme regime.


```{r make_tab_common_splist_for_hays, echo=F, results="asis"}
library(kableExtra)
library(dplyr)
tab_spinfo_hays<-as.data.frame(matrix(NA,nrow=21,ncol=3))
colnames(tab_spinfo_hays)<-c("ID","Sp. code","Species name")
tab_spinfo_hays$ID<-c(1:21)
tab_spinfo_hays$`Sp. code`<-c(as.character(spinfo$PLANTS.Symbol[which(spinfo$species %in% names(hays_spaceavg$avg.basal.cover))]),
                         "pseudo_hays")
splistfullnm<-c(names(hays_spaceavg$avg.basal.cover)[1:20],"This is a pseudo sp. = (intermediate + rare ones)")

tab_spinfo_hays$`Species name`<-c(splistfullnm)
knitr::kable(tab_spinfo_hays, 
             format="latex", align="c", linesep = "",
             caption = "Information of the most common species considered for Hays data; 20 most common species are selected based on the criterion that they had atleast 35 years of finite spatial averaged basal cover, for full list of all the species, including rare ones see Appendix \\ref{Data}.\\label{tab_spinfo_hays}", 
             booktabs = T,col.names = NULL) %>%
             column_spec(3,italic=T)%>%
             add_header_above(c("ID"=1,"Sp. code"=1,"Species name"=1))
```

```{r make_tab_common_splist_for_knz, echo=F, results="asis"}
library(kableExtra)
library(dplyr)
tab_spinfo_knz<-as.data.frame(matrix(NA,nrow=28,ncol=3))
colnames(tab_spinfo_knz)<-c("ID","Sp. code","Species name")
tab_spinfo_knz$ID<-c(1:28)
tab_spinfo_knz$`Sp. code`<-names(knz_spaceavg$avg.percent.cover)
splistfullnm<-c("Ambrosia psilostachya", "Amorpha canescens", "Andropogon gerardii", "Artemisia ludoviciana", 
                "Asclepias stenophylla", "Asclepias verticillata","Asclepias viridiflora", "Asclepias viridis",
                "Baptisia bracteata", "Bouteloua curtipendula", "Brickellia eupatorioides","Cirsium undulatum",
                "Dalea purpurea","Dichanthelium acuminatum", "Dichanthelium oligosanthes", "Lespedeza capitata",
                "Mimosa nuttallii", "Panicum virgatum", "Physalis pumila", "Ratibida columnifera",
                "Ruellia humilis", "Salvia azurea", "Schizachyrium scoparium", "Sorghastrum nutans",
                "Sporobolus compositus", "Symphyotrichum ericoides", "Vernonia baldwinii", "This is a pseudo sp.
                = (intermediate + rare ones)"
                )

tab_spinfo_knz$`Species name`<-c(splistfullnm)
knitr::kable(tab_spinfo_knz, 
             format="latex", align="c", linesep = "",
             caption = "Information of the most common species considered for KNZ data; 27 most common species are selected based on the criterion that they had 33 years of spatial averaged percent cover, for full list of all the species, including rare ones see Appendix \\ref{Data}.\\label{tab_spinfo_knz}", 
             booktabs = T,col.names = NULL) %>%
             column_spec(3,italic=T)%>%
             add_header_above(c("ID"=1,"Sp. code"=1,"Species name"=1))
```




<!--hays : npa results plot
\begin{figure}[!h]
\begin{center}
\textbf{ \hspace{0.5 cm} (A) \hspace{8 cm} (B)} \\
\includegraphics[width=8.5 cm]{./Results/hays_results/hays_npa_results/hays_spaceavg/hays_spaceavg_Corl-Coru.pdf}
\includegraphics[width=8.5 cm]{./Results/hays_results/hays_npa_results/hays_spaceavg/hays_spaceavg_Pl-Pu.pdf}\\
\vspace{1 cm}
\textbf{ \hspace{1 cm} (C) } \\
\includegraphics[width=8.5 cm]{./Results/hays_results/hays_npa_results/hays_spaceavg/hays_spaceavg_D2u-D2l.pdf}\\
\caption{ Non-parametric results for Hays space-averaged data for $(A)$ $\cor_l - \cor_u$, $(B)$ $\Ps_l - \Ps_u$ and
$(C)$ $\Dsq_u-\Dsq_l$. $nL$ and $nU$ indicate the numbers of lower tail and upper tail dependent cells (with positive correlation), respectively.\label{fig_hays_spaceavg_npa}}
\end{center}
\end{figure} -->

<!--hays : interspecific interaction : some plots-->

\begin{figure}[!h]
\begin{center}
\includegraphics[width=18 cm]{./Results/hays_results/hays_spaceavg_ANGE_PAVI2_PSTE5.pdf}
\caption[Upper tail-dependence among space-averaged basal cover of dry seasoned grasses in Hyas]
{Upper tail-dependence among space-averaged basal cover of dry seasoned grasses in Hyas; sp$_{-}$x 
and sp$_{-}$y are the id of species plotted along x- and y-axis, respectively. \label{fig_hays_spaceavg_dryseasoned_grass_UTdep}}
\end{center}
\end{figure}


\begin{figure}[!h]
\begin{center}
\includegraphics[width=14 cm]{./Results/hays_results/hays_spaceavg_SCSC_interactions.pdf}
\caption[Upper tail-dependence among space-averaged basal cover of dry seasoned grasses in Hyas]
{Upper tail-dependence among space-averaged basal cover of dry seasoned grasses in Hyas; sp$_{-}$x 
and sp$_{-}$y are the id of species plotted along x- and y-axis, respectively. Species in the y-axis 
of left column was reversed in the y-axis of right column so that species showed positive correlaion
in the copula plot; right column showed significant lower tail-dependent in pairwise copulas which was 
indicated in the red cells [(5,14), (5,12), (5,17) and (5,18)] of Fig. \ref{MT-fig_LTmUT_spaceavg_hays} 
in the maintext.\label{fig_hays_spaceavg_SCSC_interactions}}
\end{center}
\end{figure}


<!--hays & knz: time-series results plot -->
\begin{figure}[!h]
\begin{center}
\textbf{ \hspace{0.5 cm} (A) \hspace{5.5 cm} (B)} \\
\includegraphics[width=6 cm]{./Results/hays_results/skewness_results/total_timeseries.pdf}
\includegraphics[width=6 cm]{./Results/knz_results/skewness_results/total_timeseries.pdf}\\
\caption[Timeseries plot for (A) Jasper Ridge and $(B)$ Hays data]{Timeseries plot for (A) Hays and $(B)$ KNZ data; "common" species indicate the most abundant species throughout years averaging over quadrats and present minimum for $35$ years in case of Hays data. As KNZ data had relatively fewer years than Hays, so common species for KNZ indicate the species which were present for all available 33 years (1983-2015). "Rare" species refer to the species which was present maximum for 2 years in both datasets. The rest are classified into "intermediate" category.\label{fig_spaceavg_timeseries}}
\end{center}
\end{figure}







\clearpage

# References

\indent

















